# Extend from official Airflow image
FROM apache/airflow:2.8.1

# Install Java
USER root
RUN apt-get update && apt-get install -y openjdk-17-jdk unzip && apt-get clean

ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

ENV PATH=$JAVA_HOME/bin:$PATH

# Download and extract Databricks JDBC driver
RUN mkdir -p /opt/spark/jars && \
    curl -L -o /opt/spark/jars/databricks-jdbc.jar https://databricks-bi-artifacts.s3.us-east-2.amazonaws.com/simbaspark-drivers/jdbc/2.6.32/SimbaSparkJDBC42-2.6.32.1040.jar


ENV SPARK_SUBMIT_OPTIONS="--jars /opt/spark/jars/databricks-jdbc.jar"
ENV SPARK_CLASSPATH="/opt/spark/jars/*"


USER airflow

# Install PySpark and dependencies
RUN pip install pyspark==3.5.1 findspark
COPY requirements.txt /tmp/requirements.txt
RUN pip install --no-cache-dir -r /tmp/requirements.txt


